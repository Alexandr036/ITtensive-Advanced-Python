### 01 Импорт данных - Задание - получение данных по API
#### Задача
Изучите API Геокодера Яндекса
tech.yandex.ru/maps/geocoder/doc/desc/concepts/input_params-docpage/
и получите ключ API для него в кабинете разработчика.
Выполните запрос к API и узнайте долготу точки на карте (Point) для города Самара.
___Внимание:__ активация ключа Геокодера Яндекса может занимать несколько часов (до суток)._

__Метод решения:__
1. Подключим библиотеку requests и json.
2. Сформируем HTTP заголовки которые будут отправлены вместе с запросом.
3. Сформируем из документации API Геокодера Яндекса ссылку с запросом к API, где apikey - ключ разраба,  format=json - в каком формате получить, geocode=город Самара - какой город искать.
4. Отправим GET запрос через requests.get(), а также в месте с запросом передадим заголовки.
5. Через response.status_code получим код ответа (если 200 то все ОК)
6. Через json.loads загрузим ответ
7. Вычленим нужные данные из загруженного json
___
### 02 Парсинг данных - Задание - получение котировок акций
#### Задача
Получите данные по котировкам акций со страницы: mfd.ru/marketdata/?id=5&group=16&mode=3&sortHeader=name&sortOrder=1&selectedDate=01.11.2019
и найдите, по какому тикеру был максимальный рост числа сделок (в процентах) за 1 ноября 2019 года.

__Метод решения:__
1. Подключим библиотеку requests, bs4, pandas.
2. Сделаем get запрос к странице с котировками
3. Ответ загрузим в BeautifulSoup для дальнейшего поиска нужных данных
4. Найдем таблицу с тегом mfd-table и назначим ее как переменную table 
5. Переберем все значения tr найденные в table
6. По средствам get_text(strip=True) в каждом найденном td в tr уберем лишние пробелы.
7. Проверяем tr на условие, что он не пустой (len(tr)>0)
8. Добавляем не пустые в tr
9. Переименуем колонки для удобства
10. Отфильтруем наши данные, оставим только те, по которым были сделки.
11. Чтобы мы могли работать с числовыми данными, удалим '%' и заменим тире на знак минуса, а также укажим тип данных как float в колонке "Сдел.%"
12. Отсортируем по данным из  колонки "Сдел.%" в порядке убывания (ascending=False)
13. Выведем только первое значение в полученных данных, это и будет ответ на поставленную задачу. 
___

### 03 Веб-скрепинг - Задание - парсинг интернет-магазина
#### Задача
Используя парсинг данных с маркетплейса beru.ru, найдите, на сколько литров отличается общий объем холодильников Саратов 263 и Саратов 452?
Для парсинга можно использовать зеркало страницы beru.ru с результатами для холодильников Саратов по адресу: video.ittensive.com/data/018-python-advanced/beru.ru/

__Метод решения:__
1. Подключим библиотеку requests, bs4.
2. Сделаем get запрос к странице холодильниками и загрузим в BeautifulSoup.
3. Получим ссылки со страницы, поиском всех тегов начинающийся на "a".
4. Выведем все содержимое BeautifulSoup и ищем сарартов 263, находи тег с классом который будет указывать на название холодильника и ссылку на его страницу.
5. Собираем все ссылки с этим классом.
6. Теперь с помощью перебора ссылок найдем ту которая указывает на Саратов 263 и Саратов 452, обозначим их как link_263 и link_452
7. По отобранным ссылкам через BeautifulSoup найдем класс который указывает на общий объем холодильника.
8. Берем нужный нам по счету и через генератор из текста возвращаем только цифры, это и будут наши объемы.
9. Вычитаем объем 263 из 452 и получаем искомые данные.
___

### 04 Работа с SQL - Задание - загрузка результатов в БД
#### Задача
Соберите данные о моделях холодильников Саратов с маркетплейса beru.ru: URL, название, цена, размеры, общий объем, объем холодильной камеры.
Создайте соответствующие таблицы в SQLite базе данных и загрузите полученные данные в таблицу beru_goods.
Для парсинга можно использовать зеркало страницы beru.ru с результатами для холодильников Саратов по адресу: video.ittensive.com/data/018-python-advanced/beru.ru/

__Метод решения:__
1. Подключим библиотеку requests, bs4, sqlite3, pandas.
2. Сделаем get запрос к странице холодильниками и загрузим в BeautifulSoup.
3. Найдем класс тега который указывает на ссылки холодильников и соберем по нему все ссылки на холодильники.
4. Посмотрев на ссылки выяснили, что по мимо Саратова туда затесались другие, отберем только ссылки "Саратов"
5. Подключаемся в БД и создаем таблицу beru_goods в базе при помощи db.execute()
6. Теперь соберем информацию для загрузки в БД, загружая по очереди данные в BeautifulSoup из ранее полученных ссылок, собирая оттуда по заранее найденным классам название, цену, объем и размер.
7. Для нахождения данных по общему объему и объему морозильный камеры, дополнительно переберем данные по объёму и отберем те которые подходят по условиям, а также получим только числовое значение объема.
8. Полученные данные загрузим в БД
9. Загрузим из БД через pd.read_sql_query все данные, выведем их, убедимся, что все прошло корректно
10. Закроем соединение с БД
___